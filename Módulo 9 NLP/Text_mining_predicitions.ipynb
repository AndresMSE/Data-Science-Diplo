{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emchSdjSyRdX"
   },
   "source": [
    "<h1><b>Módulo 9: Procesamiento de lenguaje natural o minería de textos</b></h1>\n",
    "\n",
    "<h2><b><u>Tarea 4</u></b></h2>\n",
    "\n",
    "<b>Participantes:</b><br>\n",
    "* Marco Aurelio León Velarde\n",
    "* Nahúm Xicohtencatl Hernandez\n",
    "* Ángel Andrés Moreno Sánchez\n",
    "* Carlos Eduardo Guerrero Estrella"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5p8RxRXpvbkI"
   },
   "source": [
    "<b>Objetivo:</b> El participante identificará el conjunto de características textuales que permiten mejorar los modelos de aprendizaje supervisado para la clasificación de textos, a partir de los métodos existentes para ello y con la ayuda de las bibliotecas implementadas en Python.\n",
    "\n",
    "<b>Instrucciones:</b>\n",
    "\n",
    "El participante debe con esta actividad integrar todos los conocimientos adquiridos hasta el momento, y puede incorporar los próximos conocimientos que adquirirá durante el módulo. Debe de aplicar tareas para el preprocesamiento de textos, hacer uso de expresiones regulares, incorporar características al clasificador, aplicar los algoritmos de clasificación supervisado binaria, realizar y anotar las diferentes pruebas realizadas así como los valores de F1 como métrica de evaluación.\n",
    "\n",
    "<b>Situación a resolver:</b>\n",
    "\n",
    "El discurso de odio se define comúnmente como cualquier comunicación que menosprecia a una persona o un grupo en función de algunas características. En el año 2019 se celebró la competencia: <i>SemEval-2019 International Workshop on Semantic Evaluation</i> ( https://alt.qcri.org/semeval2019/ ) planteándose 12 tareas. De la Task 5: <i>“Multilingual detection of hate speech against immigrants and women in Twitter (hatEval)”</i> se planteó lo referente al discurso de odio en redes sociales, en específico la red social Twitter (https://competitions.codalab.org/competitions/19935 ).\n",
    "\n",
    "* Trabajar con la tarea A, dejando a libre escoger uno de los 2 idiomas.\n",
    "* Realice diferentes pruebas (mínimo 3). Anote los resultados obtenidos por cada una de ellas, y asuma diferentes características en el entrenamiento del clasificador binario.\n",
    "\n",
    "<b>Sugerencias:</b>\n",
    "\n",
    "<i>Para el preprocesamiento de los textos puede:</i>\n",
    "\n",
    "* Estandarizar el texto a minúsculas\n",
    "* Eliminar las menciones a usuarios (@user)\n",
    "* Eliminar las url’s\n",
    "* Eliminar los emojis\n",
    "* Las abreviaturas, contracciones y slangs sustituirlas por el texto equivalente\n",
    "* Eliminar palabras funcionales\n",
    "* Verificar si existen cifras numéricas, las cuales pueden ser reemplazadas por algún término o eliminarlas\n",
    "* Tratamiento con los hashtags\n",
    "* Eliminar caracteres raros y especiales\n",
    "* Eliminar signos de puntuación\n",
    "* Estandarizar las secuencias de varios espacios en blanco, tabuladores y saltos de línea\n",
    "* Entre otras…\n",
    "\n",
    "<i>Posibles características para tenerse en cuenta:</i>\n",
    "\n",
    "* N-gramas de caracteres\n",
    "* N-gramas de palabras\n",
    "* N-gramas de etiquetas POS\n",
    "* N-gramas de saltos de palabras (skip-gram)\n",
    "* N-gramas de palabras funcionales\n",
    "* N-gramas de símbolos de puntuación\n",
    "* Entre otras…\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SSjdj5ddvTDg"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leemos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_esA = pd.read_csv('public_development_en_TaskA/train_en.tsv',delimiter='\\t',encoding='utf-8')\n",
    "corpus_dev_esA = pd.read_csv('public_development_en_TaskA/dev_en.tsv',delimiter='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza en los datos\n",
    "* Cambiar todas las palabras de mayúsculas a minúsculas\n",
    "* Se han eliminado las '@' de @USUARIO con el fin de facilitar el etiquetado morfológico\n",
    "* Quitar los links \n",
    "* Quitar los emojis\n",
    "* Eliminar las stopwords\n",
    "* Se han reemplazado todos los números por el símbolo '0'\n",
    "* Quitar los signos de puntuación y quitar espacios (tabuladores, etc)\n",
    "* Utilizar los diccionarios para cambiar los slangs, contracciones y abreviaturas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_URL=\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})\"\n",
    "\n",
    "def procesar(file, namefile):    \n",
    "    file[file.columns[1]] = [clean_text(i) for i in file[file.columns[1]]]    \n",
    "    file.to_csv(namefile, sep='\\t', encoding='utf-8', index=False)\n",
    "    return file\n",
    "    \n",
    "def clean_text(text):\n",
    "    text = text.lower()   \n",
    "    #text=re.sub(\"@([A-Za-z0-9_]{1,15})\", \"@USUARIO\", text)\n",
    "    text=re.sub(\"@([A-Za-z0-9_]{1,15})\", \" \", text)\n",
    "    text=re.sub(pattern_URL, \" \", text)\n",
    "    \n",
    "    text= remove_emoji(text)\n",
    "    text= remove_stopwords(text)\n",
    "    text=re.sub(\"\\d+\", \"0\", text)\n",
    "    # text=re.sub(\"\\d+\", \" \", text)\n",
    "    \n",
    "    text=re.sub(r\" +\", \" \", re.sub(r\"\\t\", \" \", re.sub(r\"\\n+\", \"\\n\", re.sub('(?:[.,\\/!$%?¿?!¡\\^&\\*;:{}=><\\-_`~()”“\"\\'\\|])', \" \",text))))\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):    \n",
    "    stopwords=set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    for i in stopwords:\n",
    "        text = re.sub(r\"\\b%s\\b\" % i, \" \", text)\n",
    "    return text\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs                               \n",
    "                               \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               \"\\U0001F910-\\U0001F970\"  # emoticons face\n",
    "                               \"\\U00002702-\\U000027B0\"\n",
    "                               \"\\U000024C2-\\U0001F251\"\n",
    "                               \"\\U0001f926-\\U0001f937\"\n",
    "                               \"\\u200d\"\n",
    "                               \"\\u2640-\\u2642\"\n",
    "                               \"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
    "                               \"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
    "                               \"\\U0001F600-\\U0001F64F\"\n",
    "#                                r\"\\U000263A\"              # smiling face\n",
    "                               \"\\U0001F1F2\"\n",
    "                               \"\\U0001F1F4\"\n",
    "                               \"\\U0001F620\"\n",
    "                               \"\\U00010000-\\U0010ffff\"\n",
    "                               \"\\u2600-\\u2B55\"\n",
    "                               \"\\u23cf\"\n",
    "                               \"\\u23e9\"\n",
    "                               \"\\u231a\"\n",
    "                               \"\\u3030\"\n",
    "                               \"\\ufe0f\"\n",
    "                               \"]+\", flags=re.UNICODE)   \n",
    "    text = emoji_pattern.sub(r'', text) # no emoji\n",
    "    return text\n",
    "\n",
    "def get_dictionary(file):\n",
    "    dictionary = {}\n",
    "    with open(file) as file:\n",
    "         for line in file:\n",
    "                key_vals = line.split()\n",
    "                key = key_vals[0]\n",
    "                v = ''\n",
    "                for val in key_vals[1:]:\n",
    "                    v += val+' '\n",
    "                dictionary[key] = v\n",
    "    return dictionary\n",
    "\n",
    "def clean_dictionary(text, dictionary):\n",
    "    text_clean = text\n",
    "    for key,value in dictionary.items():\n",
    "        rex = re.escape(key)\n",
    "        sub = re.search('(\\w+(\\s+|[-?!]+))+',value).group()\n",
    "        text_clean = re.sub(rex,sub,text_clean)\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardamos el corpus ya procesado A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_esA = procesar(corpus_train_esA, \"public_development_en_TaskA/train_en_clean.tsv\")\n",
    "corpus_dev_esA = procesar(corpus_dev_esA, \"public_development_en_TaskA/dev_en_clean.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leyendo el corpus ya procesado\n",
    "\n",
    "train_idA = corpus_train_esA[corpus_train_esA.columns[0]]\n",
    "X_train_textA = corpus_train_esA[corpus_train_esA.columns[1]].fillna(' ')\n",
    "y_train_hsA = corpus_train_esA[corpus_train_esA.columns[2]]\n",
    "\n",
    "test_idA = corpus_dev_esA[corpus_train_esA.columns[0]]\n",
    "X_test_textA = corpus_dev_esA[corpus_dev_esA.columns[1]].fillna(' ')\n",
    "y_test_hsA = corpus_dev_esA[corpus_dev_esA.columns[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 9000\n",
      "1000 1000\n"
     ]
    }
   ],
   "source": [
    "# Longitud de train y test\n",
    "\n",
    "print( len(X_train_textA), len(y_train_hsA) )\n",
    "\n",
    "print( len(X_test_textA), len(y_test_hsA) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 4158)\n",
      "(1000, 4158)\n"
     ]
    }
   ],
   "source": [
    "# Vectorizamos \n",
    "\n",
    "cvectorizer = CountVectorizer(\n",
    "    # lowercase=True,\n",
    "    #stop_words=[word.decode('utf-8') for word in nltk.corpus.stopwords.words('spanish')],\n",
    "    #token_pattern=r'\\b\\w+\\b', #selects tokens of 2 or more alphanumeric characters \n",
    "    ngram_range=(1,3),#n-grams de palabras n = 1 a n = 3 (unigramas, bigramas y trigramas)\n",
    "    min_df=5,#ignorando los términos que tienen una frecuencia de documento estrictamente inferior a 5\n",
    ").fit(X_train_textA)\n",
    "\n",
    "X_train_cvectorized = cvectorizer.transform(X_train_textA).toarray()\n",
    "print(X_train_cvectorized.shape)\n",
    "\n",
    "X_test_cvectorized = cvectorizer.transform(X_test_textA).toarray()\n",
    "print(X_test_cvectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 4158)\n",
      "(1000, 4158)\n"
     ]
    }
   ],
   "source": [
    "tvectorizer = TfidfVectorizer(\n",
    "    # lowercase=True,\n",
    "    #stop_words=[word.decode('utf-8') for word in nltk.corpus.stopwords.words('spanish')],\n",
    "    #token_pattern=r'\\b\\w+\\b', #selects tokens of 2 or more alphanumeric characters \n",
    "    ngram_range=(1,3),#n-grams de palabras n = 1 a n = 3 (unigramas, bigramas y trigramas)\n",
    "    min_df=5,#ignorando los términos que tienen una frecuencia de documento estrictamente inferior a 5\n",
    ").fit(X_train_textA)\n",
    "\n",
    "X_train_tvectorized = tvectorizer.transform(X_train_textA).toarray()\n",
    "print(X_train_tvectorized.shape)\n",
    "\n",
    "X_test_tvectorized = tvectorizer.transform(X_test_textA).toarray()\n",
    "print(X_test_tvectorized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método 1. Perceptrón Multicapa (Multi-Layer Perceptron, MLP) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp1 = MLPClassifier(hidden_layer_sizes=(10,10,10), max_iter=500, alpha=0.0001,\n",
    "                    solver='adam', random_state=21,tol=0.000000001)\n",
    "mlp2 = MLPClassifier(hidden_layer_sizes=(6,6,6,6),solver='lbfgs',max_iter=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Accuracy mlp1 cv 0.677\n",
      "\t F1-score mlp1 cv 0.6291618828932263\n",
      "\t Accuracy mlp1 tfidv 0.664\n",
      "\t F1-score mlp1 tfidv 0.619047619047619\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mlp1.fit( X_train_cvectorized, y_train_hsA)\n",
    "predictions1 = mlp1.predict(X_test_cvectorized)\n",
    "\n",
    "print('\\t', 'Accuracy mlp1 cv', accuracy_score(y_test_hsA, predictions1))\n",
    "print('\\t', 'F1-score mlp1 cv', f1_score(y_test_hsA, predictions1))\n",
    "\n",
    "######\n",
    "\n",
    "mlp1.fit( X_train_tvectorized, y_train_hsA)\n",
    "predictions1 = mlp1.predict(X_test_tvectorized)\n",
    "\n",
    "print('\\t', 'Accuracy mlp1 tfidv', accuracy_score(y_test_hsA, predictions1))\n",
    "print('\\t', 'F1-score mlp1 tfidv', f1_score(y_test_hsA, predictions1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Accuracy mlp2 cv 0.704\n",
      "\t F1-score mlp2 cv 0.6782608695652174\n",
      "\t Accuracy mlp2 tfidv 0.573\n",
      "\t F1-score mlp2 tfidv 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mlp2.fit( X_train_cvectorized, y_train_hsA)\n",
    "predictions2 = mlp2.predict(X_test_cvectorized)\n",
    "\n",
    "print('\\t', 'Accuracy mlp2 cv', accuracy_score(y_test_hsA, predictions2))\n",
    "print('\\t', 'F1-score mlp2 cv', f1_score(y_test_hsA, predictions2))\n",
    "\n",
    "######\n",
    "\n",
    "mlp2.fit( X_train_tvectorized, y_train_hsA)\n",
    "predictions2 = mlp2.predict(X_test_tvectorized)\n",
    "\n",
    "print('\\t', 'Accuracy mlp2 tfidv', accuracy_score(y_test_hsA, predictions2))\n",
    "print('\\t', 'F1-score mlp2 tfidv', f1_score(y_test_hsA, predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizacion de los datos\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train = scaler.fit_transform(X_train_cvectorized)\n",
    "X_test = scaler.fit_transform(X_test_cvectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Accuracy mlp2 0.674\n",
      "\t F1-score mlp2 0.6393805309734514\n"
     ]
    }
   ],
   "source": [
    "mlp2.fit( X_train, y_train_hsA)\n",
    "predictions2 = mlp2.predict(X_test)\n",
    "\n",
    "print('\\t', 'Accuracy mlp2', accuracy_score(y_test_hsA, predictions2))\n",
    "print('\\t', 'F1-score mlp2', f1_score(y_test_hsA, predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No hay diferencia significativa entre normalizar o no los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método 2 Redes Neuronales Recurrentes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 4158) 9000 Secuencia de entrenamiento\n",
      "(1000, 4158) 1000 Secuencia de prueba\n"
     ]
    }
   ],
   "source": [
    "print( X_train_cvectorized.shape, len(y_train_hsA), 'Secuencia de entrenamiento' )\n",
    "\n",
    "print( X_test_cvectorized.shape, len(y_test_hsA), 'Secuencia de prueba' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 32)          320000    \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 32)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 322,113\n",
      "Trainable params: 322,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# La clase layer de redes RNN\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "# Como cualquier otra layer de Keras, SimpleRNN procesa lotes de secuencias Numpy.\n",
    "# La entrada es de la forma (batch_size, timesteps, input_features) en vez de (timesteps, input_features).\n",
    "# [muestras, pasos de tiempo, características]\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "max_features = 10000  # tamaño del diccionario de palabras comunes\n",
    "                      # (número de palabras a utilizar)\n",
    "maxlen = 1775         # longitud máxima de cada secuencia \n",
    "batch_size = 32\n",
    "\n",
    "# Capa embedding\n",
    "# input_dim : tamaño del vocabulario\n",
    "# output_dim: dimensión del vector al que se mapea\n",
    "#Se usa un embedding con tamaño de diccionario a los más de 10,000 y se mapean a dimensión un vector de dimensión 32\n",
    "model.add(Embedding(input_dim=max_features, output_dim=32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# RNN con una capa Embedding y una capa SimpleRNN que regresa solo una salida para cada secuencia\n",
    "\n",
    "# Resumen de la arquitectura\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "57/57 - 101s - loss: 0.6954 - acc: 0.5064 - val_loss: 0.6882 - val_acc: 0.7111 - 101s/epoch - 2s/step\n",
      "Epoch 2/10\n",
      "57/57 - 99s - loss: 0.6935 - acc: 0.5194 - val_loss: 0.6508 - val_acc: 0.8611 - 99s/epoch - 2s/step\n",
      "Epoch 3/10\n",
      "57/57 - 100s - loss: 0.6954 - acc: 0.4989 - val_loss: 0.6496 - val_acc: 0.8617 - 100s/epoch - 2s/step\n",
      "Epoch 4/10\n",
      "57/57 - 101s - loss: 0.6927 - acc: 0.5099 - val_loss: 0.7116 - val_acc: 0.1633 - 101s/epoch - 2s/step\n",
      "Epoch 5/10\n",
      "57/57 - 99s - loss: 0.6919 - acc: 0.5067 - val_loss: 0.6967 - val_acc: 0.1706 - 99s/epoch - 2s/step\n",
      "Epoch 6/10\n",
      "57/57 - 99s - loss: 0.6934 - acc: 0.4981 - val_loss: 0.7426 - val_acc: 0.1389 - 99s/epoch - 2s/step\n",
      "Epoch 7/10\n",
      "57/57 - 99s - loss: 0.6980 - acc: 0.4953 - val_loss: 0.5661 - val_acc: 0.8244 - 99s/epoch - 2s/step\n",
      "Epoch 8/10\n",
      "57/57 - 101s - loss: 0.7086 - acc: 0.5021 - val_loss: 0.7567 - val_acc: 0.2067 - 101s/epoch - 2s/step\n",
      "Epoch 9/10\n",
      "57/57 - 101s - loss: 0.6956 - acc: 0.5079 - val_loss: 0.6768 - val_acc: 0.7628 - 101s/epoch - 2s/step\n",
      "Epoch 10/10\n",
      "57/57 - 101s - loss: 0.6917 - acc: 0.5135 - val_loss: 0.6956 - val_acc: 0.2400 - 101s/epoch - 2s/step\n",
      "Tiempo de entrenamiento: 999.5926113128662\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "import time\n",
    "tic = time.time()\n",
    "history = model.fit(\n",
    "    X_train_cvectorized, y_train_hsA,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    verbose=2\n",
    ")\n",
    "print('Tiempo de entrenamiento:', time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 46.20%\n",
      "\t Accuracy 0.462\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X_test_cvectorized, y_test_hsA, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "# La función model.evaluate predice la salida para la entrada dada y luego calcula la función de métrica \n",
    "# especificada en model.compile y basada en y_true y y_pred y devuelve el valor de métrica calculada como salida\n",
    "\n",
    "# make predictions\n",
    "testPredict = model.predict(X_test_cvectorized)\n",
    "# model.predict simplemente devuelve el y_pred\n",
    "print('\\t', 'Accuracy', accuracy_score(y_test_hsA, testPredict.round()))\n",
    "\n",
    "# si usamos model.predict y luego calculamos las métricas uno mismo, el valor de la métrica calculada \n",
    "# debería resultar ser el mismo que model.evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método 3. Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(random_state=42, loss='exponential', learning_rate= 0.1, n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(loss='exponential', random_state=42)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb.fit(X_train_cvectorized,y_train_hsA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.713"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb.score(X_test_cvectorized, y_test_hsA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De los 3 métodos que probamos el Gradient Boosting Classifier (GBC) es el mejor ya que no sólo es el que mejor score tiene, sino el que menos costo computacional requiere."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOdLlHIs/EsVbMNOvX0BEAC",
   "collapsed_sections": [],
   "name": "M9T4_PLN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
